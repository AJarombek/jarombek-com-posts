<div>
<p>
I’ve worked with Databricks for two years at my full-time job, building ETLs and workflows using Spark and Delta Lake.
The Databricks data lakehouse is a powerful platform, supporting diverse data workloads in the cloud with simple Spark
and SQL programming techniques.  Recently, I created my own Databricks infrastructure in my AWS account, allowing me to
work with Databricks for personal applications and experimentation purposes.  This article demonstrates how to quickly
set up a Databricks workspace on AWS using Terraform.  The code for this article is available in a
<a href="https://github.com/AJarombek/databricks-spark-programs">databricks-spark-programs</a> repository on GitHub.
</p>
<SectionTitle title="Technology Overview">Technology Overview</SectionTitle>
<p>
A prerequisite for building Databricks infrastructure on AWS is an AWS account, a Databricks account, and a local
development environment with Terraform installed<sup>1</sup>.
</p>
<Definition word="AWS">
AWS (Amazon Web Services) is a cloud computing platform that provides infrastructure services such as computing power,
storage, networking, and databases.  Services are created on-demand and enable customers to deploy and operate
applications using various programming languages, tools, and frameworks. AWS also offers managed services for
higher-level functionalities, such as machine learning, artificial intelligence, and big data processing. The
platform is designed with high availability, scalability, and fault tolerance in mind, and offers a pay-as-you-go
pricing model.
</Definition>
<Definition word="Databricks">
Databricks is a cloud-based unified analytics platform that simplifies big data processing and machine learning tasks
using Apache Spark. It allows data engineers, data scientists, and business analysts to collaborate and work with
large-scale datasets in a secure and scalable environment.  Databricks integrates with Delta Lake, an open-source
storage layer that provides ACID transactions, schema enforcement, and data versioning for data lakes.  Databricks is
considered a data lakehouse because it combines the best elements of data lakes (scalable storage and processing of
structured and unstructured data) with those of data warehouses (relational and analytical capabilities for querying
and reporting) in a unified platform that supports both batch and real-time data processing.
</Definition>
<Definition word="Terraform">
Terraform is an open-source infrastructure as code (IaC) tool developed by HashiCorp that allows users to define and
manage cloud infrastructure and services in a declarative way using configuration files. It supports multiple cloud
platforms, including AWS, Microsoft Azure, Google Cloud Platform, and many others, and provides a common language and
workflow for provisioning, updating, and destroying infrastructure resources in a repeatable and automated manner.
Terraform also offers a state management system that tracks the current state of infrastructure.
</Definition>
<p>
The Databricks infrastructure, provisioned using Terraform, builds a workspace. A workspace is a cloud environment for
the Databricks data lakehouse<sup>2</sup>.  At a high level, a Databricks workspace consists of a control plane and a
data plane.  The control place is hosted by Databricks, and mainly consists of a web application to interact with the
Databricks workspace.  The data plane exists in a tenant's AWS account, and mainly consists of clusters running Apache
Spark.  The control plane "controls" the clusters within the data plane, scaling them up and down, running Spark jobs
on worker nodes, etc<sup>3</sup>.
</p>
<InlineImage filename="4-30-23-databricks-workspace-ui.png" paddingTop="true" paddingBottom="true">
</InlineImage>
<TextContext>
Databricks workspace web application home screen
</TextContext>
<p>
Within a Databricks workspace, resources are created to facilitate data workloads, such as jobs, notebooks, dashboards,
and more.  Databricks resources can be created manually through the control plane’s user interface or provisioned with
Terraform.
</p>
<SectionTitle title="Terraform Module Overview">Terraform Module Overview</SectionTitle>
<p>
My Databricks infrastructure, written using Terraform, consists of multiple modules which are located in a
<a href="https://github.com/AJarombek/databricks-spark-programs/tree/v1.0.0/infra">databricks-spark-programs GitHub
repository</a>.
</p>
<InlineImage filename="4-30-23-terraform-module-diagram.png" paddingTop="true" paddingBottom="true">
</InlineImage>
<TextContext>
Databricks Terraform Module Diagram
</TextContext>
<p>
The root module creates an entrypoint that builds all the downstream modules.  It contains two files,
<a href="https://github.com/AJarombek/databricks-spark-programs/blob/v1.0.0/infra/main.tf">main.tf</a> and
<a href="https://github.com/AJarombek/databricks-spark-programs/blob/v1.0.0/infra/var.tf">var.tf</a>.  The former
declares the Terraform backend, sets required provider versions, and invokes two downstream modules.  The latter
defines variables that are used to authenticate with a Databricks account.
</p>
<CodeSnippet language="HCL">
# main.tf

provider "aws" {
  region = "us-east-1"
}

terraform {
  required_version = ">= 1.1.2"

  required_providers {
    databricks = {
      source  = "databricks/databricks"
      version = ">= 1.6.5"
    }

    aws = {
      source  = "hashicorp/aws"
      version = ">= 4.15.0"
    }
  }

  backend "s3" {
    bucket  = "andrew-jarombek-terraform-state"
    encrypt = true
    key     = "databricks-spark-programs/infra"
    region  = "us-east-1"
  }
}

module "e2" {
  source                      = "./workspace"
  databricks_account_username = var.databricks_account_username
  databricks_account_password = var.databricks_account_password
  databricks_account_id       = var.databricks_account_id
}

module "databricks" {
  source           = "./databricks"
  databricks_host  = module.e2.databricks_host
  databricks_token = module.e2.databricks_token
}
</CodeSnippet>
<CodeSnippet language="HCL">
# var.tf

variable "databricks_account_username" {
  description = "Username for my Databricks account"
  default     = ""
  type        = string
}

variable "databricks_account_password" {
  description = "Password for my Databricks account"
  default     = ""
  type        = string
  sensitive   = true
}

variable "databricks_account_id" {
  description = "ID for my Databricks account"
  default     = ""
  type        = string
  sensitive   = true
}
</CodeSnippet>
<p>
The Databricks workspace module, named <code className="jarombek-inline-code">e2</code>, creates a Databricks workspace
and generates two output variables, <code className="jarombek-inline-code">databricks_host</code> and
<code className="jarombek-inline-code">databricks_token</code>, which are used in the
<code className="jarombek-inline-code">databricks</code> module.
</p>
<p>
The <code className="jarombek-inline-code">databricks</code> module, located in <a href="https://github.com/AJarombek/
databricks-spark-programs/tree/v1.0.0/infra/databricks">infra/databricks</a>, is a top-level module which invokes
multiple submodules, containing resources for use within a Databricks workspace.  It contains three files,
<a href="https://github.com/AJarombek/databricks-spark-programs/blob/v1.0.0/infra/databricks/main.tf">main.tf</a>,
<a href="https://github.com/AJarombek/databricks-spark-programs/blob/v1.0.0/infra/databricks/var.tf">var.tf</a>, and
<a href="https://github.com/AJarombek/databricks-spark-programs/blob/v1.0.0/infra/databricks/providers.tf">providers.tf</a>.
</p>
<CodeSnippet language="HCL">
# main.tf

module "administration" {
  source           = "./administration"
  databricks_host  = var.databricks_host
  databricks_token = var.databricks_token
}

module "notebooks" {
  source           = "./notebooks"
  databricks_host  = var.databricks_host
  databricks_token = var.databricks_token

  depends_on = [module.administration]
}

module "jobs" {
  source           = "./jobs"
  databricks_host  = var.databricks_host
  databricks_token = var.databricks_token

  depends_on = [module.administration, module.notebooks]
}

module "clusters" {
  source           = "./clusters"
  databricks_host  = var.databricks_host
  databricks_token = var.databricks_token

  depends_on = [module.administration]
}

module "storage" {
  source           = "./storage"
  databricks_host  = var.databricks_host
  databricks_token = var.databricks_token

  depends_on = [module.administration]
}
</CodeSnippet>
<CodeSnippet language="HCL">
# var.tf

variable "databricks_host" {
  description = "Host for a Databricks workspace"
  default     = ""
  type        = string
}

variable "databricks_token" {
  description = "Token for a Databricks workspace"
  default     = ""
  type        = string
  sensitive   = true
}
</CodeSnippet>
<CodeSnippet language="HCL">
# providers.tf

terraform {
  required_version = ">= 1.1.2"

  required_providers {
    databricks = {
      source  = "databricks/databricks"
      version = ">= 1.6.5"
    }
  }
}

provider "databricks" {
  host  = var.databricks_host
  token = var.databricks_token
}
</CodeSnippet>
<p>
The remaining modules include resources for configuring the Databricks workspace or populating it. First, let's
examine the module dedicated to configuring the infrastructure of the Databricks workspace.
</p>
<SectionTitle title="Workspace Infrastructure">Workspace Infrastructure</SectionTitle>
<p>
The <a href="https://github.com/AJarombek/databricks-spark-programs/tree/main/infra/workspace">workspace</a> Terraform
module creates a Databricks workspace, which serves as a collaborative environment for data engineering and data
science teams to develop, execute, and manage big data analytics and machine learning workflows using Apache Spark.
It contains multiple Terraform files, each responsible for provisioning the necessary infrastructure required for the
operation of a Databricks workspace.  The first file, <a href="https://github.com/AJarombek/databricks-spark-programs/
blob/main/infra/workspace/iam.tf">iam.tf</a>, creates a cross-account IAM role that is necessary for building a
Databricks workspace on AWS<sup>4,5</sup>.
</p>
<CodeSnippet language="HCL">
data "databricks_aws_assume_role_policy" "databricks" {
  external_id = var.databricks_account_id
}

data "databricks_aws_crossaccount_policy" "databricks" {}

resource "aws_iam_role" "cross_account_role" {
  name               = "${local.prefix}-crossaccount"
  assume_role_policy = data.databricks_aws_assume_role_policy.databricks.json
}

resource "aws_iam_role_policy" "databricks" {
  name   = "${local.prefix}-policy"
  role   = aws_iam_role.cross_account_role.id
  policy = data.databricks_aws_crossaccount_policy.databricks.json
}

resource "databricks_mws_credentials" "databricks" {
  provider         = databricks.mws
  account_id       = var.databricks_account_id
  role_arn         = aws_iam_role.cross_account_role.arn
  credentials_name = "${local.prefix}-creds"
  depends_on       = [time_sleep.wait]
}

# Without sleeping after the IAM role creation, a Terraform error occurs.
resource "time_sleep" "wait" {
  depends_on      = [aws_iam_role.cross_account_role]
  create_duration = "10s"
}
</CodeSnippet>
<SectionTitle title="Databricks Resource Infrastructure">Databricks Resource Infrastructure</SectionTitle>
<SectionTitle title="Conclusions">Conclusions</SectionTitle>
</div>
