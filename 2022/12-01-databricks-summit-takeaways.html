<div>
<p>
Databricks is a data warehouse and data lake platform designed around Apache Spark.  I use Databricks extensively at
work, but also use Databricks and Apache Spark in personal applications.  This summer, I attended my first software
engineering conference, the <a href="https://www.databricks.com/dataaisummit/">Databricks Data+AI Summit</a> in San
Francisco.  The conference was a great way to stay up-to-date on the latest trends in the data engineering industry.
</p>
<InlineImage filename="12-1-22-summit-main-stage.jpg" paddingTop="true" paddingBottom="true">
</InlineImage>
<p>
At the start, the Databricks Data+AI Summit felt more like a concert than a software engineering conference.  However,
by the end, after taking part in many breakout sessions, I had some clear takeaways about the future of the Databricks
platform and the Data Science and AI industry at large.
</p>
<SectionTitle title="Evolving Data Towards AI">Evolving Data Towards AI</SectionTitle>
<p>
Databricks is continuously adding new features in attempts to become a one-stop cloud solution for data science and AI.
Databricks isn’t alone with this vision; Snowflake is also trying to build out a platform that customers can use for
all their data science needs.  During the conference, Databricks emphasized the evolution of data science within a
company and how moving from analyzing historical data to predicting future data gives companies a competitive
edge<sup>1</sup>.  While the benefits of AI aren’t a new revelation, hearing this said at the beginning of the
conference resonated with me.  My AI programming experience is rusty at best, and is something I hope to sharpen up
soon.  As someone who works with Databricks every day at work, I should also look for opportunities to experiment with
predictive modeling.
</p>
<p>
The evolution of data science within a company was broken down into seven distinct stages<sup>2</sup>.
</p>
<ol>
<li>Clean Data</li>
<li>Reports</li>
<li>Ad Hoc Queries</li>
<li>Data Exploration</li>
<li>Predictive Modeling</li>
<li>Prescriptive Analytics</li>
<li>Automated Decision Making</li>
</ol>
<p>
Cleaning data, generating reports, ad hoc querying, and data exploration are activities I perform all the time, but
trying to find opportunities to move into stage five and beyond is equally important to iterating upon and continuously
learning within stages 1-4.  All Databricks infrastructure and data analytics code I’ve written for personal use is
available in my <a href="https://github.com/AJarombek/databricks-spark-programs">databricks-spark-programs</a>
repository, and I plan to write more about this codebase soon.
</p>
<SectionTitle title="Databricks Workflows vs Airflow">Databricks Workflows vs. Airflow</SectionTitle>
<p>
As previously mentioned, Databricks is attempting to create a one-stop cloud solution for data science and AI.  One
piece of software often used alongside Databricks is Apache Airflow, a data pipeline management tool.  I’ve written
about the basics of <a href="https://jarombek.com/blog/jan-17-2022-airflow">Apache Airflow</a> and have
<a href="https://github.com/AJarombek/data-analytics-prototypes/tree/master/Airflow">Airflow code available on GitHub</a>.
</p>
<p>
During the conference, Databricks discussed workflows, an alternative to Airflow integrated within the Databricks data
lakehouse platform.  Both Airflow and Databricks workflows are designed around orchestrating Directed Acyclic Graphs
(DAGs).  In Airflow, DAGs contain tasks and sensors of various purposes, such as running a Databricks job or waiting
for a file to exist in AWS S3.  In Databricks workflows, DAGs contain one or more Databricks jobs, which are programs
that run on a cluster within the Databricks environment.  Below is an image of a Databricks workflow containing
multiple jobs.
</p>
<InlineImage filename="12-1-22-databricks-workflow.png" paddingTop="true" paddingBottom="true">
</InlineImage>
<p>
With workflows, Databricks hopes that data engineers will orchestrate their data pipelines within the Databricks
platform instead of using a third party solution like Airflow.  Although Databricks continues to flesh out their
workflow solution, Airflow has many more features and integrates seamlessly with other cloud platforms and services.
Because Airflow is so feature rich, I expect many Databricks customers will continue using it for building data
pipelines instead of Databricks workflows until the functionality gap between the two closes.
</p>
<SectionTitle title="ELT and ETL Processes">ELT and ETL Processes</SectionTitle>
<ComparisonTable title="ELT vs. ETL">
<ComparisonTableEntry>
<h5 className="jarombek-cte-title">
Extract, Load, Transform (ELT)
</h5>
<div className="jarombek-cte-body">
<p>
An ELT is a software engineering process with three steps.  The first step extracts data from a source, the second step
loads data to a destination, and the third step transforms data within its destination locale.  Unlike an ETL, an ELT
loads raw source data into its final destination, which is likely a data warehouse, data lake, or data lakehouse.  For example, an
ELT might extract data from a source database such as PostgreSQL and load it into delta tables within Databricks, and
then transform the delta tables and save the result set within another delta table.
</p>
</div>
</ComparisonTableEntry>
<ComparisonTableEntry>
<h5 className="jarombek-cte-title">
Extract, Transform, Load (ETL)
</h5>
<div className="jarombek-cte-body">
<p>
An ETL is a software engineering process with three steps.  The first step extracts data from a source, the second step
transforms data in an intermediary location, and the third step loads data into a destination.  For example, an ETL
might extract data from a source database such as PostgreSQL, transform the data in temporary tables within the
Databricks lakehouse, and then load the data into Parquet files on AWS S3.  ETLs are a more traditional process for
working with data within a data warehouse setting, although ELTs are increasing in popularity.
</p>
</div>
</ComparisonTableEntry>
</ComparisonTable>
<p>
At the Databricks conference, there was a lot of discussion about ELTs and their potential benefits over ETLs.  The
greatest benefit of ELTs is a decoupling of the load and transform stages<sup>3</sup>.  In a traditional ETL,
transforming and loading data is often done in the same process or step of a data pipeline.  The downside of this tight
coupling is that if an error occurs during the transform stage, data won’t be loaded into the destination location.
With an ELT, data is loaded into the destination location before any transformations occur, making it available even if
there is a transformation error.  Some other benefits of ELTs include creating a raw data archive that is queryable
when business objectives change and speed since the load and transform stages can sometimes be run in
parallel<sup>4</sup>.
</p>
<p>
A potential downside of ELTs is that unneeded raw, uncleaned data is stored in the destination location, which is likely
a company's data warehouse, data lake, or data lakehouse (although as previously mentioned, some may see this as a
positive).  Storing large amounts of raw data can increase security risks and raise compliance or data governance
concerns<sup>5</sup>.
</p>
<SectionTitle title="Spark Connect">Spark Connect</SectionTitle>
<SectionTitle title="Battle for Comprehensive Data Platform Supremacy">Battle for Comprehensive Data Platform Supremacy</SectionTitle>
<SectionTitle title="Conclusions">Conclusions</SectionTitle>
</div>
