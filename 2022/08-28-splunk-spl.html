<div>
<p>
Recently at work, I’ve used Splunk Enterprise extensively to analyze application logs.  In Splunk, I’m able to query
logs, set alerts when certain scenarios occur within logs, and create dashboards to visualize logs for fellow engineers
and business analysts.  Splunk provides its own query language to interact with logs on its platform; Splunk Processing
Language (SPL) is used for searching, alerting, and dashboard creation<sup>1</sup>.
</p>
<p>
Although Splunk querying has been part of my work-life for years, I never explored it on my own or used it in any
personal projects.  Recently I decided to run a local Splunk Enterprise server to test out its most basic properties.
This groundwork may one day lead to me adopting Splunk for monitoring my applications logs.  In this article, I
describe the basics of Splunk and how to set up a local instance on Docker.  I also discuss basic SPL queries and
dashboards, and how some queries can be run before you upload any logs of your own.
</p>
<SectionTitle title="Splunk Overview">Splunk Overview</SectionTitle>
<Definition word="Splunk Enterprise">
Splunk Enterprise is software enabling log analysis on application, infrastructure, and security logs.  Splunk
consists of a web interface, query language (SPL), and command line interface<sup>2</sup>.  Users forward logs from
different sources into Splunk, where it is then indexed and made available to query.  Building upon SPL queries:
alerts, dashboards, and reports can be created.  With these tools, Splunk allows individuals and organizations to view
and react to the status of logs in real-time or near real-time<sup>3</sup>.
</Definition>
<Definition word="Centralized Log Analysis Tools">
A centralized log analysis tool, like Splunk, brings together logs from across an organization or department into a
single, searchable platform.  Centralized logging brings many quality-of-life improvements to engineers; it is
considerably easier to search on Splunk for logs across many applications with a common query language instead of
accessing individual servers where logs are initially generated.
</Definition>
<p>
Splunk is often compared to another log analysis platform called Elasticsearch, which is part of the ELK stack.  I have
written about the <a href="https://jarombek.com/blog?query=elasticsearch&page=1">ELK Stack</a> in prior articles, mostly
because it was used at my prior company.  The biggest difference between the two is Splunk requires a license while the
ELK Stack is open source.  Beyond that, I view the choice between the two platforms as a
<a href="https://www.gartner.com/reviews/market/security-information-event-management/compare/elasticsearch-vs-splunk">
personal preference</a><sup>4</sup>.
</p>
<SectionTitle title="Running Splunk Locally on Docker">Running Splunk Locally on Docker</SectionTitle>
<p>
Splunk Enterprise provides <a href="https://hub.docker.com/r/splunk/splunk/tags">official Docker images</a> on
DockerHub.  If Docker is installed and running on your computer, starting a Splunk server is as simple as the following
two shell commands.
</p>
<CodeSnippet language="Bash">
docker pull splunk/splunk:latest
docker run -d \
    -p 8000:8000 \
    -e "SPLUNK_START_ARGS=--accept-license" \
    -e "SPLUNK_PASSWORD=splunkadmin" \
    --name splunk \
    splunk/splunk:latest
</CodeSnippet>
<p>
It may take a minute for the container to be ready.  With the Docker container running, navigating to
<strong>localhost:8080</strong> in a web browser should show the following sign-in screen.
</p>
<InlineImage filename="8-28-22-splunk-sign-in.png" paddingTop="true" paddingBottom="true">
</InlineImage>
<p>
Sign in with the credentials configured in the shell command: <strong>admin</strong> for the username and
<strong>splunkadmin</strong> for the password.  After signing in, you are presented with the homepage for Splunk
Enterprise!
</p>
<InlineImage filename="8-28-22-splunk-homepage.png" paddingTop="true" paddingBottom="true">
</InlineImage>
<SectionTitle title="Basic SPL Queries and Commands">Basic SPL Queries and Commands</SectionTitle>
<p>
Now we are ready to start using Splunk.  Before forwarding any logs to Splunk, we can utilize Splunk’s default indexes
to practice writing simple queries.  An index in Splunk is a repository of data (logs), and the act of indexing
transforms raw data sitting in files at an index into searchable events<sup>5,6</sup>.  One of these indexes present
on every Splunk installation by default is <code className="jarombek-inline-code">_internal</code>.  From the Splunk
homepage, clicking on "Search & Reporting" on the left-hand sidebar presents the search screen.  From here, entering
the following query will retrieve events from the <code className="jarombek-inline-code">_internal</code> index.
</p>
<CodeSnippet language="SPL">
index=_internal
</CodeSnippet>
<InlineImage filename="8-28-22-splunk-query.png" paddingTop="true" paddingBottom="true">
</InlineImage>
<p>
On the left-hand side of the screen, a list of fields are shown which exist in events and are queryable on the current
index.  A slightly more complex query might perform statistical analysis on one of these fields.  The following query
aggregates the events and checks the count of each unique value in the
<code className="jarombek-inline-code">source</code> field.
</p>
<CodeSnippet language="SPL">
index=_internal
| stats count by source
</CodeSnippet>
<InlineImage filename="8-28-22-splunk-count-query.png" paddingTop="true" paddingBottom="true">
</InlineImage>
<p>
In the second line of the query above, <code className="jarombek-inline-code">stats</code> is an SPL command which
performs statistical aggregations<sup>7</sup>.  <code className="jarombek-inline-code">count</code> is a function for
counting the number of elements, <code className="jarombek-inline-code">by</code> is a clause to group the aggregation
function by, and <code className="jarombek-inline-code">source</code> is the name of a field.  All together, the
command <code className="jarombek-inline-code">stats count by source</code> means "perform a statistical aggregation,
grouped by the source field, where the number of events are counted."
</p>
<p>
Another basic query technique is to filter an index to specific events.  One way to accomplish this is the
<code className="jarombek-inline-code">where</code> SPL command.  For those experienced with SQL, the following query
has the same functionality as a <code className="jarombek-inline-code">WHERE expression LIKE pattern</code> statement.
</p>
<CodeSnippet language="SPL">
index=_internal
| where source LIKE "%health.log"
</CodeSnippet>
<InlineImage filename="8-28-22-splunk-filter-query.png" paddingTop="true" paddingBottom="true">
</InlineImage>
<p>
A more complex Splunk query may chain together multiple statements using pipes, similar in functionality to Unix pipes
in shell commands.  For example, the following SPL command pipes together multiple statements, returning all the
prior run SPL queries on the Splunk server.
</p>
<CodeSnippet language="SPL">
index=_audit
| where action="search"
| where isnotnull(search)
| sort -_time
| table _time, search
</CodeSnippet>
<InlineImage filename="8-28-22-splunk-prior-queries.png" paddingTop="true" paddingBottom="true">
</InlineImage>
<p>
Queries can also be used to display visualizations, such as the following query which charts memory utilization by
Splunk over time.
</p>
<CodeSnippet language="SPL">
index="_introspection"
| where isnotnull("data.pct_memory") AND component="PerProcess"
| chart max(data.pct_memory) by _time
</CodeSnippet>
<InlineImage filename="8-28-22-splunk-memory-chart.png" paddingTop="true" paddingBottom="true">
</InlineImage>
<p>
The goal of showing these queries is not to make you an expert at Splunk, but to provide a general sense of how Splunk
querying works.  Complex queries can be written and experimented with before any custom indexes are created, which is
great for beginners wishing to experiment.  <a href="https://docs.splunk.com/Documentation/SplunkCloud/8.2.2203/
SearchReference/WhatsInThisManual">Splunk documentation</a> provides lots of great information on how to structure
queries along with documentation on each SPL command<sup>8</sup>.
</p>
<SectionTitle title="Querying Application Logs">Querying Application Logs</SectionTitle>
<p>
In order to query application logs, a custom index must be created.  In a production environment, Splunk forwarders
are used to send logs to the Splunk Enterprise server where they are placed in an index.  For demonstration purposes,
we can skip the setup of a forwarder and manually upload log data to Splunk.  To do this, navigate to the homepage
and click on "Add Data".
</p>
<InlineImage filename="8-28-22-splunk-add-data.png" paddingTop="true" paddingBottom="true">
</InlineImage>
<p>
On the following page, scroll down and click "Upload files from my computer".
</p>
<InlineImage filename="8-28-22-splunk-upload-files.png" paddingTop="true" paddingBottom="true">
</InlineImage>
<p>
A multi-step process is required to upload a file manually.  For full details, check out the
<a href="https://docs.splunk.com/Documentation/Splunk/latest/SearchTutorial/GetthetutorialdataintoSplunk">
uploading tutorial data</a> article provided by Splunk<sup>9</sup>.  In my scenario, I uploaded a sample log file from
my <a href="https://github.com/AJarombek/saints-xctf-api">saints-xctf-api</a> application (accessible from
<a href="https://api.saintsxctf.com/">api.saintsxctf.com</a>).  This sample log is available in a
<a href="https://github.com/AJarombek/devops-prototypes/blob/v1.0.1/splunk/logs/saints-xctf-api.log">
saints-xctf-api.log</a> file in my <a href="https://github.com/AJarombek/devops-prototypes">devops-prototypes</a>
repository.
</p>
<p>
While uploading my log file, I also created an index for its data to be stored within.  This index, called
<code className="jarombek-inline-code">saints_xctf_api</code>, is queryable the same as the default indexes I showed
before.
</p>
<CodeSnippet language="SPL">
source="saints-xctf-api.log" index="saints_xctf_api"
</CodeSnippet>
<InlineImage filename="8-28-22-splunk-custom-index.png" paddingTop="true" paddingBottom="true">
</InlineImage>
<p>
Even with a small log file like <strong>saints-xctf-api.log</strong>, interesting queries can be run, such as the
following one which calculates the number of requests based on HTTP status codes.  More queries that can be run on
this log file are available in a <a href="https://github.com/AJarombek/devops-prototypes/blob/master/splunk/
saints_xctf_api_queries.spl">saints_xctf_api_queries.spl</a> file on GitHub.
</p>
<CodeSnippet language="SPL">
source="saints-xctf-api.log" index="saints_xctf_api"
| rex field=_raw "] \"(?&lt;verb>[A-Z]+) (?&lt;endpoint>.+) (?&lt;http_version>.+)\" (?&lt;status>[0-9]+)"
| table _time verb, endpoint, http_version, status
| where isnotnull(endpoint)
| stats count by status
</CodeSnippet>
<InlineImage filename="8-28-22-splunk-http-codes.png" paddingTop="true" paddingBottom="true">
</InlineImage>
<SectionTitle title="Splunk Dashboards">Splunk Dashboards</SectionTitle>
<p>
One of the great advantages of a centralized logging tool is the ability to create visualizations for data, potentially
from multiple different sources and applications.  In Splunk, multiple visualizations can be combined together into a
dashboard.  Dashboards are great to analyze logs and application logs quickly.  They also help business analysts with
limited or no programming background to gain insight into the state of the processes they manage.
</p>
<p>
Splunk Enterprise has a "Dashboard" tab from which dashboards are created, edited, and viewed.  I usually create
dashboards using the newer "Dashboard Studio" format, which backs each dashboard with JSON code.  Dashboards are
edited using no-code in the UI or by manually editing their JSON code.  Usually, development of dashboards beyond
the simplest examples require a combination of both approaches.
</p>
<InlineImage filename="8-28-22-splunk-dashboards.png" paddingTop="true" paddingBottom="true">
</InlineImage>
<p>
The following dashboard, saved in a <a href="https://github.com/AJarombek/devops-prototypes/blob/v1.0.1/splunk/
splunk_internals.json">splunk_internals.json</a> file, uses two SPL queries to display information about Splunk memory
utilization and previously run queries.
</p>
<InlineImage filename="8-28-22-splunk-internal-dashboard.png" paddingTop="true" paddingBottom="true">
</InlineImage>
<p>
Using the JSON files, dashboards can be passed from one Splunk environment to another.  For example, using the
<strong>splunk_internals.json</strong> file, my dashboard can be easily replicated.  To do this, first create a new
dashboard on Splunk Enterprise, and then paste the JSON code into the "Source" section of the dashboard.
</p>
<InlineImage filename="8-28-22-splunk-dashboard-create.png" paddingTop="true" paddingBottom="true">
</InlineImage>
<InlineImage filename="8-28-22-splunk-dashboard-source.png" paddingTop="true" paddingBottom="true">
</InlineImage>
<p>
There is a lot more to show with dashboards, such as how to add custom styling in JSON code.  I will try to cover
that in a future article, but for newcomers to Splunk, it’s important to begin by leveraging the basic no-code
capabilities of Splunk dashboards to meet business needs.
</p>
<SectionTitle title="Conclusions">Conclusions</SectionTitle>
<p>
Splunk is a great way to centralize and perform analysis on logs from a multitude of sources.  Creating dashboards to
display commonly used queries is a great way to provide engineers and business analysts with insights into data and
application state.  All the code shown in this article is available on <a href="https://github.com/AJarombek/
devops-prototypes/tree/v1.0.1/splunk">GitHub</a>.
</p>
</div>