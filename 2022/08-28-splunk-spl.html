<div>
<p>
Recently at work, I’ve used Splunk Enterprise extensively to analyze application logs.  In Splunk, I’m able to query
logs, trigger alerts when scenarios occur, and create dashboards to visualize logs for fellow engineers
and business analysts.  Splunk provides its own query language to interact with logs on its platform; Splunk Processing
Language (SPL) is used for searching, alerting, and dashboard creation<sup>1</sup>.
</p>
<p>
Although Splunk querying was part of my work-life for years, I never explored it on my own or used it in any
personal projects.  Recently, I decided to run a local Splunk Enterprise server to experiment with its basic properties.
This groundwork may lead to my adoption of Splunk for monitoring applications logs.  In this article, I
describe the basics of Splunk and how to set up a local instance on Docker.  I also discuss basic SPL queries and
dashboards, and how some queries can be run before you upload any logs of your own.
</p>
<SectionTitle title="Splunk Overview">Splunk Overview</SectionTitle>
<Definition word="Splunk Enterprise">
Splunk Enterprise is software that enables log analysis on application, infrastructure, and security logs.  Splunk
consists of a web interface, query language (SPL), and command line interface<sup>2</sup>.  Users forward logs from
different sources onto Splunk, where they are indexed and made available to query.  Alerts, dashboards, and reports can be
created on Splunk, which build upon SPL queries.  With these tools, Splunk allows individuals and organizations to view
and react to the status of logs in real-time or near real-time<sup>3</sup>.
</Definition>
<Definition word="Centralized Log Analysis Tools">
A centralized log analysis tool brings together logs from across an organization or department into a
single, searchable platform.  Splunk is a popular example of a centralized log analysis tool.  Centralized logging
brings many quality-of-life improvements to engineers; it is
considerably easier to search on Splunk for logs using a common query language instead of
accessing individual servers where logs are initially generated.
</Definition>
<p>
Splunk is often compared to another log analysis platform called Elasticsearch, which is part of the ELK stack.  I have
written about the <a href="https://jarombek.com/blog?query=elasticsearch&page=1">ELK Stack</a> in prior articles, mostly
because it was used at my prior company.  The biggest difference between the two is Splunk requires a license while the
ELK Stack is open source.  Beyond that, I view the choice between the two platforms as a
<a href="https://www.gartner.com/reviews/market/security-information-event-management/compare/elasticsearch-vs-splunk">
personal preference</a><sup>4</sup>.
</p>
<SectionTitle title="Running Splunk Locally on Docker">Running Splunk Locally on Docker</SectionTitle>
<p>
Splunk Enterprise provides <a href="https://hub.docker.com/r/splunk/splunk/tags">official Docker images</a> on
DockerHub.  If Docker is installed and running on your computer, starting a Splunk server is as simple as the following
two shell commands.
</p>
<CodeSnippet language="Bash">
docker pull splunk/splunk:latest
docker run -d \
    -p 8000:8000 \
    -e "SPLUNK_START_ARGS=--accept-license" \
    -e "SPLUNK_PASSWORD=splunkadmin" \
    --name splunk \
    splunk/splunk:latest
</CodeSnippet>
<p>
It may take a minute for the container to be ready.  With the Docker container running, navigating to
<strong>localhost:8080</strong> in a web browser should show the following sign-in screen.
</p>
<InlineImage filename="8-28-22-splunk-sign-in.png" paddingTop="true" paddingBottom="true">
</InlineImage>
<p>
Sign in with the credentials configured in the shell command: <strong>admin</strong> for the username and
<strong>splunkadmin</strong> for the password.  After signing in, you are presented with the homepage for Splunk
Enterprise!
</p>
<InlineImage filename="8-28-22-splunk-homepage.png" paddingTop="true" paddingBottom="true">
</InlineImage>
<SectionTitle title="Basic SPL Queries and Commands">Basic SPL Queries and Commands</SectionTitle>
<p>
Now we are ready to start using Splunk.  Before forwarding any logs to Splunk, Splunk’s default indexes can be utilized
 to practice writing simple queries.  An index in Splunk is a repository of data (i.e. logs), and the act of indexing
transforms raw data sitting in files at an index into searchable events<sup>5,6</sup>.  One index present
on every Splunk installation is <code className="jarombek-inline-code">_internal</code>.  From the Splunk
homepage, clicking on "Search & Reporting" on the left-hand side presents a search screen.  From here, entering
the following query will retrieve events from the <code className="jarombek-inline-code">_internal</code> index.
</p>
<CodeSnippet language="SPL">
index=_internal
</CodeSnippet>
<InlineImage filename="8-28-22-splunk-query.png" paddingTop="true" paddingBottom="true">
</InlineImage>
<p>
On the left-hand side of the screen, a list of fields (under "Selected Fields" and "Interesting Fields") are shown which exist in events and are queryable on the current
index.  A slightly more complex query might perform statistical analysis on one of these fields.  The following query
aggregates the events and checks the count of each unique value in the
<code className="jarombek-inline-code">source</code> field.
</p>
<CodeSnippet language="SPL">
index=_internal
| stats count by source
</CodeSnippet>
<InlineImage filename="8-28-22-splunk-count-query.png" paddingTop="true" paddingBottom="true">
</InlineImage>
<p>
In the second line of the query above, <code className="jarombek-inline-code">stats</code> is an SPL command which
performs statistical aggregations<sup>7</sup>.  <code className="jarombek-inline-code">count</code> is an aggregation function for
counting the number of events, <code className="jarombek-inline-code">by</code> is a clause to group the aggregation
function (similar to a SQL <code className="jarombek-inline-code">GROUP BY</code> statement), and
<code className="jarombek-inline-code">source</code> is the name of a field to group upon.  All together, the
command <code className="jarombek-inline-code">stats count by source</code> means "perform a statistical aggregation,
grouped by the source field, where the number of events are counted."
</p>
<p>
Another basic query technique is to filter events within an index.  One way to accomplish filtering is with the
<code className="jarombek-inline-code">where</code> SPL command.  For those experienced with SQL, the following query
has the same functionality as a <code className="jarombek-inline-code">WHERE &lt;expression> LIKE &lt;pattern></code> statement.
</p>
<CodeSnippet language="SPL">
index=_internal
| where source LIKE "%health.log"
</CodeSnippet>
<InlineImage filename="8-28-22-splunk-filter-query.png" paddingTop="true" paddingBottom="true">
</InlineImage>
<p>
A more complex Splunk query may chain together multiple statements using pipes, similar in functionality to Unix pipes
in shell commands.  For example, the following SPL command pipes together multiple statements, returning all
 previously run SPL queries on the Splunk server.
</p>
<CodeSnippet language="SPL">
index=_audit
| where action="search"
| where isnotnull(search)
| sort -_time
| table _time, search
</CodeSnippet>
<InlineImage filename="8-28-22-splunk-prior-queries.png" paddingTop="true" paddingBottom="true">
</InlineImage>
<p>
Queries are also used to display visualizations, such as the following query which charts Splunk's memory utilization
over time.
</p>
<CodeSnippet language="SPL">
index="_introspection"
| where isnotnull("data.pct_memory") AND component="PerProcess"
| chart max(data.pct_memory) by _time
</CodeSnippet>
<InlineImage filename="8-28-22-splunk-memory-chart.png" paddingTop="true" paddingBottom="true">
</InlineImage>
<p>
Understanding these queries alone will not to make you an expert at Splunk, but they should provide a general sense of how
 SPL queries are used.  Complex queries can be written and experimented with before any custom indexes are created, which is
great for beginners wishing to experiment.  <a href="https://docs.splunk.com/Documentation/SplunkCloud/8.2.2203/
SearchReference/WhatsInThisManual">Splunk documentation</a> provides great information on how to structure
queries along with documentation on each SPL command<sup>8</sup>.
</p>
<SectionTitle title="Querying Application Logs">Querying Application Logs</SectionTitle>
<p>
In order to query application logs, a custom index must be created.  In a production environment, Splunk forwarders
are used to send logs to a Splunk Enterprise server where they are placed in an index.  For demonstration purposes,
 I'll skip the setup of a forwarder and manually upload log data to Splunk.  To do this, start by navigating to the homepage
and click on "Add Data".
</p>
<InlineImage filename="8-28-22-splunk-add-data.png" paddingTop="true" paddingBottom="true">
</InlineImage>
<p>
On the following page, scroll down and click "Upload files from my computer".
</p>
<InlineImage filename="8-28-22-splunk-upload-files.png" paddingTop="true" paddingBottom="true">
</InlineImage>
<p>
A multi-step process is required to upload a file manually.  For full details, check out the
<a href="https://docs.splunk.com/Documentation/Splunk/latest/SearchTutorial/GetthetutorialdataintoSplunk">
uploading tutorial data</a> article provided by Splunk<sup>9</sup>.  In my scenario, I uploaded a sample log file from
my <a href="https://github.com/AJarombek/saints-xctf-api">saints-xctf-api</a> application (accessible from
<a href="https://api.saintsxctf.com/">api.saintsxctf.com</a>).  This sample log is available in a
<a href="https://github.com/AJarombek/devops-prototypes/blob/v1.0.1/splunk/logs/saints-xctf-api.log">
saints-xctf-api.log</a> file in my <a href="https://github.com/AJarombek/devops-prototypes">devops-prototypes</a>
repository.
</p>
<p>
While uploading my log file, I also created a corresponding Splunk index.  This index, called
<code className="jarombek-inline-code">saints_xctf_api</code>, is queryable just like the default indexes.
</p>
<CodeSnippet language="SPL">
source="saints-xctf-api.log" index="saints_xctf_api"
</CodeSnippet>
<InlineImage filename="8-28-22-splunk-custom-index.png" paddingTop="true" paddingBottom="true">
</InlineImage>
<p>
Even with a small log file like <strong>saints-xctf-api.log</strong>, interesting queries can be run, such as the
following which calculates the frequency of each HTTP status code returned by the API.  Additional queries I ran on
this log file are available in a <a href="https://github.com/AJarombek/devops-prototypes/blob/master/splunk/
saints_xctf_api_queries.spl">saints_xctf_api_queries.spl</a> file on GitHub.
</p>
<CodeSnippet language="SPL">
source="saints-xctf-api.log" index="saints_xctf_api"
| rex field=_raw "] \"(?&lt;verb>[A-Z]+) (?&lt;endpoint>.+) (?&lt;http_version>.+)\" (?&lt;status>[0-9]+)"
| table _time verb, endpoint, http_version, status
| where isnotnull(endpoint)
| stats count by status
</CodeSnippet>
<InlineImage filename="8-28-22-splunk-http-codes.png" paddingTop="true" paddingBottom="true">
</InlineImage>
<SectionTitle title="Splunk Dashboards">Splunk Dashboards</SectionTitle>
<p>
One of the great advantages of a centralized logging tool is the ability to create visualizations for data, potentially
from multiple different sources and applications.  In Splunk, one or more visualizations can be combined together into a
dashboard.  Dashboards provide a great way to analyze application logs quickly, and help business analysts with
limited or no programming background to gain insight into the state of the processes they manage.
</p>
<p>
Splunk Enterprise has a "Dashboard" tab from which dashboards are created, edited, and viewed.  I usually create
dashboards using the newer "Dashboard Studio" format, which implements each dashboard with JSON code.  Dashboards are
 mainly edited using no-code tools in the UI, but can also be manually modified by changing their JSON configurations.
  Usually, highly customized dashboards require a combination of both approaches.
</p>
<InlineImage filename="8-28-22-splunk-dashboards.png" paddingTop="true" paddingBottom="true">
</InlineImage>
<p>
The following dashboard, saved in a <a href="https://github.com/AJarombek/devops-prototypes/blob/v1.0.1/splunk/
splunk_internals.json">splunk_internals.json</a> file, uses two SPL queries to display information about Splunk memory
utilization and previously run queries.
</p>
<InlineImage filename="8-28-22-splunk-internal-dashboard.png" paddingTop="true" paddingBottom="true">
</InlineImage>
<p>
Using JSON files, dashboards can be passed from one Splunk environment to another.  For example, using the
<strong>splunk_internals.json</strong> file, my dashboard can be easily replicated.  To do this, first create a new
dashboard on Splunk Enterprise, and then paste the JSON code into the "Source" section of the dashboard (accessed via
the button outlined in red).
</p>
<InlineImage filename="8-28-22-splunk-dashboard-create.png" paddingTop="true" paddingBottom="true">
</InlineImage>
<InlineImage filename="8-28-22-splunk-dashboard-source.png" paddingTop="true" paddingBottom="true">
</InlineImage>
<p>
There is a lot more to show with dashboards, such as how to add custom styling in JSON code.  I will try to cover
that in a future article, but for newcomers to Splunk, it’s important to begin by leveraging the basic no-code
capabilities of Splunk dashboards to meet business needs.
</p>
<SectionTitle title="Conclusions">Conclusions</SectionTitle>
<p>
Splunk is a great way to centralize and perform analysis on logs from multiple sources.  Creating dashboards to
display commonly used queries is a great way to provide engineers and business analysts with insights into data and
application state.  All the code shown in this article is available on <a href="https://github.com/AJarombek/
devops-prototypes/tree/v1.0.1/splunk">GitHub</a>.
</p>
</div>