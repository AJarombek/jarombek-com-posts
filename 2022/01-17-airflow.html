<div>
<p>
Over the last six months, I’ve been using Apache Airflow extensively at work.  Airflow is a platform and framework for
building and automating data pipelines<sup>1</sup>.  Airflow data pipelines are written in Python and interoperate with
many different technologies, such as databases, cloud platforms, containers, and more.  Often, Airflow is used in
realms of data analytics and machine learning.
</p>
<p>
While Airflow data pipelines are written in Python, the software they automate and schedule do not need to be Python
related.  However, the fact that Python is used makes Airflow data pipelines highly configurable and customizable.
Since Python is very popular and simpler to learn compared to other languages, most engineers will be able to work with
Airflow easily.
</p>
<p>
There are three main objectives to this article: introducing the basic concepts of Airflow, creating an Airflow
development environment, and exploring basic Airflow pipelines.  The code discussed in this article is available on
<a href="https://github.com/AJarombek/data-analytics-prototypes/tree/master/Airflow">GitHub</a>.
</p>
<SectionTitle title="Airflow Concepts">Airflow Concepts</SectionTitle>
<Definition word="Airflow">
Airflow is a platform and framework for building, automating, and scheduling data pipelines<sup>2</sup>.  Data
pipelines in Airflow are known as workflows or Directed Acyclic Graphs (DAGs).  Airflow DAGs are configured as code
using Python, and can be run ad hoc or on a schedule.  The Airflow platform creates an execution and scheduling
environment for DAGs, viewable from a web interface.
</Definition>
<p>
The main objective of Airflow is to run DAGs either manually or based on a schedule.
</p>
<Definition word="Airflow DAG">
A Directed Acyclic Graph (DAG), also referred to as a data pipeline or workflow, is a schedulable graph of tasks
which is run based on a schedule.  Since DAGs are acyclic, the graph of tasks in a DAG can’t contain any cycles, but
can branch and converge as needed.  Airflow DAGs are written and configured in Python.  DAGs contain information such
as a list of tasks, the execution order (graph) of the tasks, an execution schedule, and additional metadata.
</Definition>
<p>
An image of an Airflow DAG, as seen from the Airflow UI, is shown below.
</p>
<InlineImage filename="1-17-22-airflow-dag.png" paddingTop="true" paddingBottom="true">
</InlineImage>
<Definition word="Airflow Task">
In Airflow, tasks are the smallest unit of execution<sup>3</sup>.  Tasks are units within a DAG, with upstream and
downstream dependencies within the graph.  Tasks can perform simple operations such as running a Python function or
Bash script, or more complex operations like running a Docker container.  There are many different types of tasks,
which are created using Airflow Operators.  Operators are templates for building tasks; for example, a
<code className="jarombek-inline-code">PythonOperator</code> is used to create a task that runs a Python
function<sup>4</sup>.
</Definition>
<p>
The Airflow platform consists of multiple components; most importantly, Airflow consists of a web server, scheduler,
and metastore.
</p>
<ComparisonTable title="Airflow Components">
<ComparisonTableEntry>
<h5 className="jarombek-cte-title">
Web Server
</h5>
<div className="jarombek-cte-body">
<p>
In Airflow, the web server is the UI in which users can view and trigger DAGs. The web server is also helpful for
viewing DAG run results and debug DAGs by looking through execution logs.
</p>
</div>
</ComparisonTableEntry>
<ComparisonTableEntry>
<h5 className="jarombek-cte-title">
Scheduler
</h5>
<div className="jarombek-cte-body">
<p>
The Airflow scheduler is a constantly running program that monitors all DAGs in the Airflow environment<sup>5</sup>.
Python DAG files exist in a specific directory in the Airflow environment, and the scheduler is responsible for parsing
these DAG files and storing information about them within the Airflow metastore.  The scheduler also checks if DAGs and
tasks within DAGs are eligible for execution.  When tasks are eligible for execution, the scheduler places them in a
queued state, and then executes them<sup>6</sup>.  In many ways, the scheduler is the heart of the Airflow platform.
</p>
</div>
</ComparisonTableEntry>
<ComparisonTableEntry>
<h5 className="jarombek-cte-title">
Metastore
</h5>
<div className="jarombek-cte-body">
<p>
The Airflow metastore holds metadata about an Airflow environment, including configuration details and DAG information.
Everything that happens within the Airflow environment also exists in the metastore, including DAG run information.
The metastore is a relational database, commonly MySQL or PostgreSQL.  <a href="https://www.astronomer.io/guides/
airflow-database">This article</a> provides a good high-level overview of the data stored in the metastore.
</p>
</div>
</ComparisonTableEntry>
</ComparisonTable>
<SectionTitle title="Airflow Webserver Tour">Airflow Webserver Tour</SectionTitle>
<p>
After signing into the Airflow UI, the initial view displays all the DAGs in the Airflow environment.
</p>
<InlineImage filename="1-17-22-airflow-home.png" paddingTop="true" paddingBottom="true">
</InlineImage>
<p>
The list of DAGs displays basic information about each DAG, such as their execution schedules, and the results of
recent runs.  It also gives options to toggle DAGs on and off (the switch to the left of the DAG name) and run DAGs
(the play button in the "Actions" column).  Clicking on a DAG shows the following view:
</p>
<InlineImage filename="1-17-22-airflow-graph-view.png" paddingTop="true" paddingBottom="true">
</InlineImage>
<p>
Airflow DAGs have multiple views; the view shown above is called the Graph view.  The Graph view shows the DAG and the
result of the previous run.  In this case, both tasks ran successfully, as denoted by both tasks being outlined in
green.  Hovering over a task supplies more information about it, and clicking on the task provides options such as
viewing the logs or re-running the task.
</p>
<InlineImage filename="1-17-22-airflow-graph-view-hover.png" paddingTop="true" paddingBottom="true">
</InlineImage>
<InlineImage filename="1-17-22-airflow-graph-view-click.png" paddingTop="true" paddingBottom="true">
</InlineImage>
<p>
Clicking on the "Log" button displays the logs for the task run, which is very useful for debugging.
</p>
<InlineImage filename="1-17-22-airflow-log-view.png" paddingTop="true" paddingBottom="true">
</InlineImage>
<p>
Another useful page is the DAGs tree view.  This page shows the results of all the prior DAG runs.  In the image below,
the last two runs of the <code className="jarombek-inline-code">hello_world</code> DAG are shown, both of which were
successful.
</p>
<InlineImage filename="1-17-22-airflow-tree-view.png" paddingTop="true" paddingBottom="true">
</InlineImage>
<p>
Airflow provides many ways to view DAGs and environment configurations, but the pages shown above are the ones I’ve
found most useful these past six months.
</p>
<SectionTitle title="Airflow Development Environment">Airflow Development Environment</SectionTitle>
<p>
While it is possible to run Airflow on the host machine of your development environment, a more elegant approach is to
use Docker.  With Docker, you have an Airflow environment that works across different operating systems and is started
with a single command.  No Airflow dependencies are needed on your host machine with this approach.  Since Airflow
often has a complex setup with multiple containers, I use Docker Compose to orchestrate them.
</p>
<p>
I’ve created multiple Airflow development environments of varying degrees of complexity.  The major difference between
these environments comes down to the executor, which is the component of Airflow which runs scheduled tasks.  The three
development environments in my <a href="https://github.com/AJarombek/data-analytics-prototypes/tree/master/Airflow">
data-analytics-prototypes</a> repository utilize <a href="https://github.com/AJarombek/data-analytics-prototypes/tree/
master/Airflow/sequential-executor">sequential</a>, <a href="https://github.com/AJarombek/data-analytics-prototypes/
tree/master/Airflow/local-executor">local</a>, and <a href="https://github.com/AJarombek/data-analytics-prototypes/
tree/master/Airflow/celery-executor">celery</a> executors.
</p>
<p>
The simplest executor is the sequential executor, which is not recommended for production usage.  Even for development
use it can become a bottleneck, because it runs tasks sequentially, one at a time.  However, when you are just getting
started, the sequential executor is likely sufficient.
</p>
<p>
The configuration for the sequential executor local environment consists of a <a href="https://github.com/AJarombek/
data-analytics-prototypes/blob/master/Airflow/sequential-executor/Dockerfile">Dockerfile</a> and a
<a href="https://github.com/AJarombek/data-analytics-prototypes/blob/master/Airflow/sequential-executor/
docker-compose.yml">docker-compose.yml</a> file.  The contents of these files are shown below.
</p>
<CodeSnippet language="Dockerfile">
# Dockerfile

FROM apache/airflow:2.2.0-python3.8

RUN airflow db init \
    && airflow users create --username admin --password admin --firstname Anonymous --lastname Admin --role Admin --email admin@example.org
</CodeSnippet>
<CodeSnippet language="YAML">
# docker-compose.yml

version: '3.8'

x-environment: &airflow_environment
  - AIRFLOW__CORE__EXECUTOR=SequentialExecutor
  - AIRFLOW__CORE__LOAD_EXAMPLES=False
  - AIRFLOW__CORE__STORE_DAG_CODE=True
  - AIRFLOW__CORE__STORE_SERIALIZED_DAGS=True
  - AIRFLOW__WEBSERVER__EXPOSE_CONFIG=True
  - _PIP_ADDITIONAL_REQUIREMENTS=apache-airflow-providers-postgres==2.3.0

services:
  airflow:
    build:
      dockerfile: Dockerfile
      context: .
    environment: *airflow_environment
    ports:
      - "8080:8080"
    volumes:
      - logs:/opt/airflow/logs
      - ../dags:/opt/airflow/dags
    networks:
      - airflow-net
    entrypoint: /bin/bash
    command: -c 'airflow webserver & airflow scheduler'

volumes:
  logs:

networks:
  airflow-net:
    driver: bridge
</CodeSnippet>
<p>
The Docker Compose file runs a container based on the Dockerfile.  With this setup, a simple
<code className="jarombek-inline-code">docker-compose up</code> command from the command line will start the Airflow
server.
</p>
<p>
Let’s go over these files in a bit more detail.  The <strong>Dockerfile</strong> uses an official Airflow image,
<code className="jarombek-inline-code">apache/airflow:2.2.0-python3.8</code>, as its base image.  The
<code className="jarombek-inline-code">RUN</code> command initializes the Airflow metastore database
(<code className="jarombek-inline-code">airflow db init</code>) and creates a user that can sign into the Airflow
webserver (<code className="jarombek-inline-code">airflow users create</code>).  For the sequential executor, the
Airflow metastore uses SQLite as its database engine.
</p>
<p>
The <strong>docker-compose.yml</strong> file runs a single <code className="jarombek-inline-code">airflow</code> service
using the <strong>Dockerfile</strong>, as specified by <code className="jarombek-inline-code">dockerfile: Dockerfile
</code>.  The <code className="jarombek-inline-code">airflow</code> service starts a container, executing the
<code className="jarombek-inline-code">airflow webserver & airflow scheduler</code> command.  This command starts the
Airflow webserver in the background and the Airflow scheduler in the foreground.  The Airflow webserver is exposed on
port <strong>8080</strong>, and accessible when run locally at <strong>http://localhost:8080/</strong>.
</p>
<p>
There are two volumes attached to the container.  The first is a location to hold Airflow logs, specified by
<code className="jarombek-inline-code">logs:/opt/airflow/logs</code>.  The second is a location in my local filesystem
holding Airflow DAGs, specified by <code className="jarombek-inline-code">../dags:/opt/airflow/dags</code>.  The
relative path from my <a href="https://github.com/AJarombek/data-analytics-prototypes/tree/master/Airflow/
sequential-executor">docker-compose.yml</a> file to my <a href="https://github.com/AJarombek/data-analytics-prototypes/
tree/master/Airflow/dags">dags</a> directory is <strong>../dags</strong>.  This <strong>dags</strong> directory is
mounted on the container within <strong>/opt/airflow/dags</strong>.  This directory is where the Airflow scheduler reads
DAGs from.
</p>
<SectionTitle title="Basic Airflow Pipelines">Basic Airflow Pipelines</SectionTitle>
<SectionTitle title="Conclusions">Conclusions</SectionTitle>
</div>
